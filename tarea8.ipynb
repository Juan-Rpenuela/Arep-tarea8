{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d3ae810",
   "metadata": {},
   "source": [
    "# Sistema RAG con LangChain y Pinecone\n",
    "\n",
    "Este notebook implementa un sistema de **Retrieval-Augmented Generation (RAG)** que permite realizar consultas inteligentes sobre documentos web utilizando:\n",
    "- **LangChain**: Framework para aplicaciones con LLMs\n",
    "- **OpenAI GPT-4**: Modelo de lenguaje para generación de respuestas\n",
    "- **Pinecone**: Base de datos vectorial para búsqueda semántica\n",
    "- **BeautifulSoup4**: Parsing de contenido HTML\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a1e52d",
   "metadata": {},
   "source": [
    "## 1. Instalación de Dependencias\n",
    "\n",
    "En esta sección instalamos todas las librerías necesarias para el proyecto:\n",
    "- **openai**: Cliente de OpenAI para acceso a GPT-4 y embeddings\n",
    "- **python-dotenv**: Gestión de variables de entorno\n",
    "- **langchain**: Framework principal para RAG\n",
    "- **langchain-text-splitters**: Herramientas para dividir documentos\n",
    "- **langchain-community**: Loaders de documentos (WebBaseLoader)\n",
    "- **bs4 (BeautifulSoup)**: Parser HTML\n",
    "- **langchain-openai**: Integración específica de LangChain con OpenAI\n",
    "- **langchain-pinecone**: Integración con Pinecone Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70d95bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839ab4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain-text-splitters langchain-community bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e1229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U \"langchain[openai]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a88f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U \"langchain-openai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5296dfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fa1058",
   "metadata": {},
   "source": [
    "## 2. Importación de Librerías\n",
    "\n",
    "Importamos todas las dependencias necesarias organizadas por funcionalidad:\n",
    "1. **Variables de entorno y cliente OpenAI**\n",
    "2. **Modelos de chat y embeddings**\n",
    "3. **Pinecone vector store**\n",
    "4. **Web scraping y document loaders**\n",
    "5. **Text splitters para chunking**\n",
    "6. **Tools y agents de LangChain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6bd920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar las dependencias de env\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43044d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b5d532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "77807d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "564c2173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6677486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain.agents import create_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f874de",
   "metadata": {},
   "source": [
    "## 3. Configuración Inicial\n",
    "\n",
    "### 3.1 Variables de Entorno y Cliente OpenAI\n",
    "Cargamos las credenciales desde el archivo `.env`:\n",
    "- `OPENAI_API_KEY`: Para acceso a GPT-4 y embeddings\n",
    "- `PINECONE_API_KEY`: Para acceso a Pinecone\n",
    "- `PINECONE_INDEX_NAME`: Nombre del índice vectorial\n",
    "- `LANGCHAIN_API_KEY`: Para tracing (opcional)\n",
    "- `LANGCHAIN_TRACING`: Activar monitoreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c3adc844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cliente inicializado. Modelo listo para consultas.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()  # Lee el archivo .env\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "langchain_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "langchain_tracing = os.getenv(\"LANGCHAIN_TRACING\")\n",
    "\n",
    "\n",
    "print(\"Cliente inicializado. Modelo listo para consultas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84546a1d",
   "metadata": {},
   "source": [
    "### 3.2 Inicialización de Modelos\n",
    "Configuramos los modelos de IA:\n",
    "- **GPT-4.1**: Modelo de lenguaje para generación de respuestas\n",
    "- **text-embedding-3-large**: Modelo de embeddings con 1024 dimensiones para vectorización de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0b2ace96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_chat_model(\"gpt-4.1\")\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\",dimensions=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4421ec9a",
   "metadata": {},
   "source": [
    "### 3.3 Configuración de Pinecone Vector Store\n",
    "Conectamos con Pinecone y creamos el vector store:\n",
    "- Inicializamos el cliente de Pinecone con la API key\n",
    "- Conectamos al índice existente\n",
    "- Creamos el PineconeVectorStore que usará los embeddings de OpenAI para indexar y buscar documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "54b07bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "index = pc.Index(os.getenv(\"PINECONE_INDEX_NAME\"))\n",
    "\n",
    "vector_store = PineconeVectorStore(embedding=embeddings, index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19e6eb5",
   "metadata": {},
   "source": [
    "## 4. Carga y Procesamiento de Documentos\n",
    "\n",
    "### 4.1 Carga de Documento Web\n",
    "Utilizamos `WebBaseLoader` para extraer contenido de un blog post:\n",
    "- **URL**: Blog de Lilian Weng sobre agentes de IA\n",
    "- **Parser**: BeautifulSoup con filtro para extraer solo título, headers y contenido\n",
    "- **Resultado**: Un documento con todo el texto relevante del post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e57b10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 43047\n"
     ]
    }
   ],
   "source": [
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 1\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ccb951",
   "metadata": {},
   "source": [
    "### 4.2 Visualización del Contenido\n",
    "Mostramos los primeros 500 caracteres del documento cargado para verificar que se extrajo correctamente el contenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "52efbc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecda934",
   "metadata": {},
   "source": [
    "### 4.3 División de Documentos en Chunks\n",
    "Utilizamos `RecursiveCharacterTextSplitter` para dividir el documento en fragmentos manejables:\n",
    "- **Chunk Size**: 1000 caracteres por fragmento\n",
    "- **Chunk Overlap**: 200 caracteres de superposición entre chunks para mantener contexto\n",
    "- **add_start_index**: Rastrea la posición original en el documento\n",
    "- **Propósito**: Fragmentos pequeños mejoran la precisión de la búsqueda semántica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88058e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 63 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dcc365",
   "metadata": {},
   "source": [
    "### 4.4 Indexación en Pinecone\n",
    "Agregamos los documentos fragmentados al vector store:\n",
    "- Cada chunk se convierte en un embedding de 1024 dimensiones\n",
    "- Los embeddings se almacenan en Pinecone junto con los metadatos\n",
    "- Retorna los IDs de los documentos indexados\n",
    "- **Nota**: `add_documents()` puede retornar `None` en algunas versiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fac81ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2e7d2b65-7738-4fe2-a041-e526140778a7', 'bc6bf7d7-8c0f-4914-9a17-f1c656482adc', 'e40d98b6-ee17-4889-931c-a6e4ac4d7998']\n"
     ]
    }
   ],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e388973",
   "metadata": {},
   "source": [
    "## 5. Creación del Agente RAG\n",
    "\n",
    "### 5.1 Definición de la Herramienta de Recuperación\n",
    "Creamos una herramienta personalizada que:\n",
    "- Recibe una query del usuario\n",
    "- Realiza búsqueda de similitud en Pinecone (top k=2)\n",
    "- Retorna tanto el contenido serializado como los documentos originales\n",
    "- Usa `response_format=\"content_and_artifact\"` para devolver ambos formatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f047969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ad3f81",
   "metadata": {},
   "source": [
    "### 5.2 Configuración del Agente\n",
    "Creamos el agente con:\n",
    "- **Modelo**: GPT-4.1\n",
    "- **Tools**: La función `retrieve_context` definida anteriormente\n",
    "- **System Prompt**: Instrucciones para que el agente use la herramienta de recuperación al responder consultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7b671986",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retrieve_context]\n",
    "# If desired, specify custom instructions\n",
    "prompt = (\n",
    "    \"You have access to a tool that retrieves context from a blog post. \"\n",
    "    \"Use the tool to help answer user queries.\"\n",
    ")\n",
    "agent = create_agent(model, tools, system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066d748c",
   "metadata": {},
   "source": [
    "## 6. Ejecución de Consultas\n",
    "\n",
    "### 6.1 Consulta de Ejemplo\n",
    "Ejecutamos una consulta compleja que requiere:\n",
    "1. Buscar información sobre \"Task Decomposition\"\n",
    "2. Una vez obtenida la respuesta, buscar extensiones de ese método\n",
    "3. El agente decide cuándo usar la herramienta de recuperación\n",
    "4. Combina la información recuperada para generar una respuesta coherente\n",
    "\n",
    "**Modo de streaming**: Las respuestas se muestran en tiempo real a medida que se generan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d911fad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the standard method for Task Decomposition?\n",
      "\n",
      "Once you get the answer, look up common extensions of that method.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_context (call_I5d25JSTu1QXuulxcB9Cu0d5)\n",
      " Call ID: call_I5d25JSTu1QXuulxcB9Cu0d5\n",
      "  Args:\n",
      "    query: standard method for Task Decomposition\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_context (call_I5d25JSTu1QXuulxcB9Cu0d5)\n",
      " Call ID: call_I5d25JSTu1QXuulxcB9Cu0d5\n",
      "  Args:\n",
      "    query: standard method for Task Decomposition\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578.0}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578.0}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578.0}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578.0}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_context (call_PJcUB5NUoAunWfvruDvMBpTd)\n",
      " Call ID: call_PJcUB5NUoAunWfvruDvMBpTd\n",
      "  Args:\n",
      "    query: common extensions of task decomposition methods\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_context (call_PJcUB5NUoAunWfvruDvMBpTd)\n",
      " Call ID: call_PJcUB5NUoAunWfvruDvMBpTd\n",
      "  Args:\n",
      "    query: common extensions of task decomposition methods\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578.0}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578.0}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578.0}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578.0}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The standard method for Task Decomposition typically involves:\n",
      "\n",
      "1. Using a large language model (LLM) with simple prompts such as \"Steps for XYZ.\\n1.\" or \"What are the subgoals for achieving XYZ?\" \n",
      "2. Providing task-specific instructions like \"Write a story outline\" for novel writing.\n",
      "3. Incorporating human input, where people break down the task manually.\n",
      "\n",
      "Common extensions to these methods include:\n",
      "\n",
      "- Using LLM+P, which combines language models with external classical planners for more complex, long-horizon planning. This approach uses the Planning Domain Definition Language (PDDL) as a bridge between the LLM and a classical planner, translating the problem into PDDL, getting a plan from the planner, and then translating back into natural language.\n",
      "- In LLM+P, the planning step is essentially outsourced to an external tool, which is useful when there are domain-specific PDDL files and planners available (commonly in robotics and similar fields).\n",
      "\n",
      "In summary, task decomposition usually starts with prompting or instructions, but more advanced setups can integrate classical planning tools and formal languages like PDDL for greater complexity and automation.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The standard method for Task Decomposition typically involves:\n",
      "\n",
      "1. Using a large language model (LLM) with simple prompts such as \"Steps for XYZ.\\n1.\" or \"What are the subgoals for achieving XYZ?\" \n",
      "2. Providing task-specific instructions like \"Write a story outline\" for novel writing.\n",
      "3. Incorporating human input, where people break down the task manually.\n",
      "\n",
      "Common extensions to these methods include:\n",
      "\n",
      "- Using LLM+P, which combines language models with external classical planners for more complex, long-horizon planning. This approach uses the Planning Domain Definition Language (PDDL) as a bridge between the LLM and a classical planner, translating the problem into PDDL, getting a plan from the planner, and then translating back into natural language.\n",
      "- In LLM+P, the planning step is essentially outsourced to an external tool, which is useful when there are domain-specific PDDL files and planners available (commonly in robotics and similar fields).\n",
      "\n",
      "In summary, task decomposition usually starts with prompting or instructions, but more advanced setups can integrate classical planning tools and formal languages like PDDL for greater complexity and automation.\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    \"What is the standard method for Task Decomposition?\\n\\n\"\n",
    "    \"Once you get the answer, look up common extensions of that method.\"\n",
    ")\n",
    "\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
